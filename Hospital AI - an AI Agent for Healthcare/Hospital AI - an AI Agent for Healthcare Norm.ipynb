{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c443fd44-7feb-409b-a28e-cd39a6a51a32",
   "metadata": {},
   "source": [
    "# üöÄ Hospital AI - an AI Agent for Healthcare Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00f705-7857-4b52-8932-a521aa5c96a4",
   "metadata": {},
   "source": [
    "## üìñ Project Overview\n",
    "This project aims to leverage artificial intelligence to enhance key areas of hospital operations and patient care. By applying machine learning and data analysis, the system is designed to improve efficiency, accuracy, and overall patient outcomes.\n",
    "- **Objective**: Develop a specialized AI agent to provide accurate and up-to-date information on healthcare norms, regulations, and health insurance standards.\n",
    "- **Methodology**: The project will involve fine-tuning a language model on a curated dataset of official documents, legal texts, and policy manuals specific to the healthcare and insurance sectors.\n",
    "- **Core Functionality**: The agent will be able to interpret complex healthcare rules, standards, and guidelines based on the provided documents.\n",
    "- **Primary Goal**: To create an intelligent system that assists stakeholders, such as patients, healthcare providers, or administrators, in navigating the complexities of health insurance policies and ensuring compliance.\n",
    "- **Expected Outcome**: The agent will deliver clear, actionable insights and serve as a reliable resource for verifying information and adhering to established healthcare and insurance standards.\n",
    "\n",
    "This project builds a **Retrieval-Augmented Generation (RAG) system** using a combination of:\n",
    "- **Vector Database (FAISS)** for semantic search  \n",
    "- **Embedding Model** (`sentence-transformers/all-MiniLM-L6-v2`)  \n",
    "- **Lightweight LLM** (`google/flan-t5-small`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c31cd-39c0-450d-8ae7-22c0a903a1cc",
   "metadata": {},
   "source": [
    "### üõ† Steps Implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f06f9-f77d-4fe8-bc0a-c35d50741e73",
   "metadata": {},
   "source": [
    "### ‚úÖ üìÇ 1. Load & Import Libraries\n",
    "Installed and imported the following libraries for initial serup\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "- os\n",
    "- pdfplumber\n",
    "- nltk\n",
    "- pathlib\n",
    "- json\n",
    "- openai\n",
    "- tqdm\n",
    "- transformers\n",
    "- torch\n",
    "- peft\n",
    "- re\n",
    "- datasets\n",
    "- faiss\n",
    "- pickle\n",
    "- sentence_transformers\n",
    "- Defined the project: **Hospital AI** ‚Äì An AI agent for hospital norms, safety protocols, and insurance rules.\n",
    "- Installed required libraries: `transformers`, `datasets`, `peft`, `accelerate`, `bitsandbytes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12af3246-1965-4994-98f3-4dcc65c3a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, TrainingArguments, get_scheduler, BitsAndBytesConfig, LlamaTokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, AutoModel     \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6bde6-6e74-4d25-b143-592194374585",
   "metadata": {},
   "source": [
    "### ‚úÖ 2. Data Preparation\n",
    "- Collected guidelines and rules from healthcare sources (hospital norms, safety protocols, and insurance documents).\n",
    "- Structured data in **JSONL format** with fields: `instruction`, `input`, `output`.\n",
    "- Preprocessed dataset and created training/evaluation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21056518-67ce-473d-b1b8-2764bb15d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_DIR = \"Hospital AI Dataset\"   # folder path with PDFs\n",
    "CHUNK_SIZE = 400   # words per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "011b885b-3826-4424-8673-6214401d3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=CHUNK_SIZE):\n",
    "    words = text.split()\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield \" \".join(words[i:i+chunk_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70ecb986-0a6f-4b90-af35-8ed677746386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hospital_ai_1.pdf: 128 chunks, 51102 words\n",
      "hospital_ai_3.pdf: 341 chunks, 136088 words\n",
      "hospital_ai_2.pdf: 13 chunks, 4845 words\n",
      "hospital_ai_6.pdf: 120 chunks, 47750 words\n",
      "hospital_ai_5.pdf: 474 chunks, 189393 words\n",
      "hospital_ai_4.pdf: 18 chunks, 6866 words\n",
      "hospital_ai_10.pdf: 133 chunks, 53028 words\n",
      " hospital_ai_7.pdf: 121 chunks, 48046 words\n",
      "hospital_ai_9.pdf: 7 chunks, 2638 words\n",
      "hospital_ai_8.pdf: 21 chunks, 8305 words\n"
     ]
    }
   ],
   "source": [
    "pdf_chunks = {}  # {filename: [chunks]}\n",
    "for pdf_file in Path(PDF_DIR).glob(\"*.pdf\"):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "\n",
    "    chunks = list(chunk_text(text))\n",
    "    pdf_chunks[pdf_file.name] = chunks\n",
    "    print(f\"{pdf_file.name}: {len(chunks)} chunks, {len(text.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86b4ffe-8f90-4470-a194-4154da377bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed67ce-de38-43f4-8b70-6f492c7f7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"  # replace with your token\n",
    "\n",
    "# ==========================\n",
    "# 1) Define a function to generate JSONL from a chunk\n",
    "# ==========================\n",
    "def generate_jsonl_from_chunk(chunk_text, filename, section_number):\n",
    "    \"\"\"\n",
    "    Send a chunk of text to the model and get JSON-formatted instruction-output examples.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an AI dataset generator for instruction-tuned LLMs.\n",
    "\n",
    "Text from PDF chunk:\n",
    "\\\"\\\"\\\"{chunk_text}\\\"\\\"\\\"\n",
    "\n",
    "Your task:\n",
    "1. Generate as many instruction-input-output examples as possible (up to 50000 if you can).\n",
    "2. Output MUST be a valid JSON list of objects.\n",
    "3. Each object MUST have the keys:\n",
    "   - \"instruction\": a question/task based on PDF content\n",
    "   - \"input\": context or relevant info from the PDF\n",
    "   - \"output\": correct answer/explanation\n",
    "4. DO NOT include any text outside the JSON list.\n",
    "\n",
    "Each example's input should reference the filename and section number like:\n",
    "\"From PDF {filename}, section {section_number}\"\n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",        # or gpt-5 if available\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=2000,      # adjust depending on chunk size\n",
    "    )\n",
    "\n",
    "    raw_output = response['choices'][0]['message']['content']\n",
    "\n",
    "    try:\n",
    "        data = json.loads(raw_output)\n",
    "        if isinstance(data, list):\n",
    "            # Make sure input field contains filename + section\n",
    "            for ex in data:\n",
    "                ex[\"input\"] = f\"From PDF {filename}, section {section_number}: \" + ex.get(\"input\", \"\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Unexpected format in section {section_number} of {filename}\")\n",
    "            return [{\"instruction\": \"Parse failure, raw output\",\n",
    "                     \"input\": f\"From PDF {filename}, section {section_number}\",\n",
    "                     \"output\": raw_output}]\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"‚ö†Ô∏è JSON parse failed for section {section_number} of {filename}\")\n",
    "        return [{\"instruction\": \"Parse failure, raw output\",\n",
    "                 \"input\": f\"From PDF {filename}, section {section_number}\",\n",
    "                 \"output\": raw_output}]\n",
    "\n",
    "# ==========================\n",
    "# 2) Loop over pdf_chunks and generate dataset\n",
    "# ==========================\n",
    "dataset = []\n",
    "\n",
    "for fname, chunks in pdf_chunks.items():\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=f\"Processing {fname}\")):\n",
    "        examples = generate_jsonl_from_chunk(chunk, fname, i+1)\n",
    "        dataset.extend(examples)\n",
    "\n",
    "# ==========================\n",
    "# 3) Save dataset as JSONL\n",
    "# ==========================\n",
    "output_file = \"ci_cd_dataset.jsonl\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in dataset:\n",
    "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved {len(dataset)} examples into {output_file}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dec46c9-9508-4c4b-bdab-8c0c79df2d04",
   "metadata": {},
   "source": [
    "### ‚úÖ 3. Embedding Model\n",
    "- Loaded **MiniLM (all-MiniLM-L6-v2)** for sentence embeddings.\n",
    "- Converted each `output` into a **vector representation**.\n",
    "- Stored embeddings + metadata (`instruction`, `output`) in FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b2a7c-9586-4940-861d-f6cb17ab1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ==========================\n",
    "# 1) Configs\n",
    "# ==========================\n",
    "model_name = \"google/flan-t5-small\"\n",
    "dataset_path = \"ci_cd_dataset.jsonl\"\n",
    "max_length = 512\n",
    "batch_size = 2\n",
    "num_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# ==========================\n",
    "# 2) Load dataset\n",
    "# ==========================\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# ==========================\n",
    "# 3) Load tokenizer\n",
    "# ==========================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ==========================\n",
    "# 4) Load model first\n",
    "# ==========================\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "\n",
    "# ==========================\n",
    "# 5) Apply LoRA\n",
    "# ==========================\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],  # T5-specific\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ==========================\n",
    "# 6) Tokenize dataset correctly\n",
    "# ==========================\n",
    "def tokenize_fn(examples):\n",
    "    # Combine instruction and input into a single string for each example\n",
    "    # Use a simple f-string for clarity and to handle cases with or without input\n",
    "    inputs = [\n",
    "        f\"{instruction}\\n{input_text}\" if input_text else instruction\n",
    "        for instruction, input_text in zip(examples[\"instruction\"], examples[\"input\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize the combined inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenize the outputs to create the labels\n",
    "    labels = tokenizer(\n",
    "        examples[\"output\"],\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Set the labels, ensuring the padding token is ignored during loss calculation\n",
    "    tokenized_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    tokenized_inputs[\"labels\"][labels[\"input_ids\"] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "test_dataset = tokenized_dataset[\"test\"]\n",
    "\n",
    "# ==========================\n",
    "# 7) DataLoader + DataCollator\n",
    "# ==========================\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "eval_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "\n",
    "# ==========================\n",
    "# 8) Optimizer\n",
    "# ==========================\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# ==========================\n",
    "# 9) Training loop\n",
    "# ==========================\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        # Move all tensors to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# ==========================\n",
    "# 10) Save LoRA-adapted model\n",
    "# ==========================\n",
    "model.save_pretrained(\"./flan_t5_lora_mac\")\n",
    "tokenizer.save_pretrained(\"./flan_t5_lora_mac\")\n",
    "\n",
    "# ==========================\n",
    "# Inference Cell (works now)\n",
    "# ==========================\n",
    "# Config\n",
    "model_path = \"./flan_t5_lora_mac\"  # where you saved after training\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "# ==========================\n",
    "# 1) Standardized Prompt Builder\n",
    "# ==========================\n",
    "def build_prompt(user_prompt: str, retrieved_context: str = \"\") -> str:\n",
    "    if retrieved_context and retrieved_context.strip():\n",
    "        return f\"\"\"You are an AI assistant with prior knowledge from your training. \n",
    "You also have access to external information retrieved from a knowledge base (vector DB). \n",
    "\n",
    "Use BOTH:\n",
    "1. Your trained knowledge  \n",
    "2. The retrieved context below  \n",
    "\n",
    "to provide the best, most accurate, and helpful response to the user‚Äôs question.\n",
    "\n",
    "User Question:\n",
    "{user_prompt}\n",
    "\n",
    "Retrieved Context:\n",
    "{retrieved_context}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"You are an AI assistant with prior knowledge from your training. \n",
    "\n",
    "Answer the following question as best as possible using your trained knowledge.\n",
    "\n",
    "User Question:\n",
    "{user_prompt}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "# ==========================\n",
    "# 2) Generate Response Function\n",
    "# ==========================\n",
    "def generate_response(user_prompt: str, retrieved_context: str = \"\", max_new_tokens: int = 200) -> str:\n",
    "    prompt = build_prompt(user_prompt, retrieved_context)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# ==========================\n",
    "# 3) Example Usage\n",
    "# ==========================\n",
    "#retrieved_context = the chunk(s) of text you pulled from your vector DB (e.g., FAISS, Pinecone, Chroma) that are most relevant to the user‚Äôs query.\n",
    "#user_prompt = the actual question or instruction the user gave.\n",
    "\n",
    "retrieved_context = \"\"\"\n",
    "Ensure that In cases of transfer or evacuation of patients, the \n",
    "Medical Director, or designee, will direct and monitor the movement of patients\n",
    "\"\"\"\n",
    "\n",
    "# Example usage with your generate_response function\n",
    "print(generate_response(\n",
    "    \"What are the standard hand hygiene procedures for nurses and doctors in hospitals?\", \n",
    "    retrieved_context\n",
    "))\n",
    "\n",
    "print(generate_response(\n",
    "    \"Translate this into French:\", \n",
    "    \"Continuous Integration and Continuous Deployment (CI/CD) helps automate testing and deployment for ML pipelines.\"\n",
    "))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e8773-a8f5-4515-86d1-7ac9de461120",
   "metadata": {},
   "source": [
    "### ‚úÖ 4. Vector Database Creation and Retrieval Function\n",
    "- Built a FAISS index for efficient similarity search.\n",
    "- Metadata is stored in parallel to retrieve the **exact output text** when queried.\n",
    "- A function retrieves **top-k similar outputs** from FAISS given a new query.\n",
    "- Ensures semantic similarity, not just keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d49729a-cf5a-4e29-93df-02fe261ff545",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = \"hospital_ai_dataset.jsonl\"   # your dataset file\n",
    "outputs = []\n",
    "with open(jsonl_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        outputs.append(obj[\"output\"])   # only take output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd97139e-64db-4329-8e4f-5446e9984521",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embedder.encode(outputs, convert_to_numpy=True)\n",
    "\n",
    "dimension = embeddings.shape[1]  # embedding size\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "id_to_output = {i: outputs[i] for i in range(len(outputs))}\n",
    "\n",
    "def query_vector_db(query, k=2):\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    return [id_to_output[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23dbf3df-1db5-48d1-9fa6-2235de62cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results from vector DB:\n",
      "- - The hospital environment: how clean should a hospital be?\n",
      "- The hospital policies and procedures should address, at a minimum:\n"
     ]
    }
   ],
   "source": [
    "query = \"Instruction: What are the hospital hygiene norms?\"\n",
    "results = query_vector_db(query, k=2)\n",
    "\n",
    "print(\"Top results from vector DB:\")\n",
    "for r in results:\n",
    "    print(\"-\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a10f85-2176-4ec8-854e-9d34e48bd704",
   "metadata": {},
   "source": [
    "### ‚úÖ 5. Model Selection, Fine-Tuning, Model Training and LLM Integration (RAG) and Answer Generation\n",
    "- Started with lightweight model: **`google/flan-t5-small`** for prototyping.\n",
    "- Planned scaling to larger models (Vicuna, LLaMA, Falcon) with **LoRA/QLoRA**.\n",
    "- Loaded **FLAN-T5-small (~300MB)** as the reasoning model.\n",
    "- Configured training arguments (batch size, epochs, learning rate).\n",
    "- Implemented tokenization and prompt formatting.\n",
    "- Fine-tuned the model on the custom dataset.\n",
    "- Saved model and tokenizer in `./hospital_ai_flant5_lora`.\n",
    "- - Constructed a **prompt** combining:\n",
    "  - User query  \n",
    "  - Retrieved context from vector DB  \n",
    "  - Explicit instructions on how to refine, optimize, or correct the answer.\n",
    "- If Vector DB contains relevant info ‚Üí LLM optimizes and explains it.  \n",
    "- If Vector DB info is incomplete ‚Üí LLM generates a correct, standalone answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00184ff7-573d-4dba-8ae0-95e333832cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "llm_model_name = \"google/flan-t5-base\"\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "llm_model.to(device)\n",
    "llm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a650fb0-4a9f-4871-8a99-31ea012f0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vector_db(query, embedder, index, id_to_output, k=3):\n",
    "    query_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "    results = [id_to_output[i] for i in indices[0]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a0d6489-1b3f-4134-a0ad-6740b09b472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(query, embedder, index, id_to_output, llm_model, llm_tokenizer, k=3):\n",
    "    # Retrieve relevant \"outputs\" from vector DB\n",
    "    retrieved_outputs = retrieve_from_vector_db(query, embedder, index, id_to_output, k=k)\n",
    "    context = \"\\n\".join(retrieved_outputs)\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt = f\"\"\"\n",
    "You are an expert assistant.\n",
    "\n",
    "User query:\n",
    "{query}\n",
    "\n",
    "Relevant information retrieved from the knowledge base:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- If the context already fully answers the query, summarize it concisely.\n",
    "- If the context is partial, improve/complete it with your own knowledge.\n",
    "- Keep the answer clear and professional.\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate answer\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(**inputs, max_new_tokens=200)\n",
    "    answer = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer, retrieved_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605e9854-978f-47ce-a919-ca92b2464616",
   "metadata": {},
   "source": [
    "### ‚úÖ 6. Inference\n",
    "- Loaded base + LoRA adapters for inference.\n",
    "- Generated outputs for hospital norms and insurance-related queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf73781b-ed0d-40a0-b44d-99ba57ddac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_ai_fintuned_llm_vectordb_output(user_query = ''):\n",
    "    llm_response, context = answer_with_rag(\n",
    "        user_query,\n",
    "        embedder,          # the sentence-transformers model\n",
    "        index,\n",
    "        id_to_output,\n",
    "        llm_model,\n",
    "        llm_tokenizer,\n",
    "        k=2\n",
    "    )\n",
    "    \n",
    "    '''\n",
    "    print(\"LLM Response:\")\n",
    "    print(llm_response)\n",
    "    print('\\n')\n",
    "    print(\"Vector DB Response:\")\n",
    "    print(vectordb_response)\n",
    "    '''\n",
    "    print(\"LLM + VectorDB Response:\")\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ea2e1-b723-4617-afcd-aef07c9fd312",
   "metadata": {},
   "source": [
    "### ‚úÖ 7. Example Questions Tested\n",
    "- \"What are the standard hand hygiene procedures for nurses?\"\n",
    "- \"What documentation is required for filing a hospital insurance claim?\"\n",
    "- \"What are the standard protocols for sterilizing surgical instruments?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39437577-ea51-4147-bd43-5c68a6962525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Guideline for hand hygiene in health\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What are the standard hand hygiene procedures for nurses and doctors in hospitals?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36f59805-8fac-4b89-b033-4b9102f488a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "EPA guide for infectious waste management.\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What is the protocol for handling infectious waste in a hospital?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3030cea-c04c-4870-a773-5dce66843c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Ensure that In cases of transfer or evacuation of patients, the \n",
      "Medical Director, or designee, will direct and monitor the movement of patients\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What safety measures should be taken when moving a patient with limited mobility?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0e80246-67d3-49f2-9031-0f002a25fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Ensure that 1) Either a full operative or procedure report, or a brief operative or procedure note \n",
      "must be documented immediately following surgery or a procedure (inpatient or \n",
      "11 \n",
      " outpatient) that requires anesthesia, or deep or moderate sedation before the \n",
      "patient is transferred from the operating room or procedure room to the next level of \n",
      "care\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What are the standard procedures for monitoring a patient after surgery?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a4778f4-fc18-46e7-8e85-c2b289c0154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "The hospital‚Äôs policies and procedures are expected to address how hospital staff who play a role in facilitating or controlling visitor access to patients will be trained to assure \n",
      "appropriate implementation of the visitation policies and procedures and a voidance of \n",
      "unnecessary restrictions or limitations on patients‚Äô visitation rights.\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What are the rules for visitor access in an ICU?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65d65876-0e53-4b0c-9f5a-8bb584606069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Ensure that Hospitals must establish policies and procedures for reporting of medication errors, ADRs, and incompatibilities, and ensure that staff is aware of the reporting process\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What are the reporting requirements for a medical error in a hospital?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44123f07-4af2-480f-aa33-c3bcabd975b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "- What are Some Types of Health Care Coverage?\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'Which procedures are typically covered under standard health insurance in the US?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a6dbb20-8766-4ec6-9e9a-527cff783bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "The hospital must maintain documentation of\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What documentation is required for filing a hospital insurance claim?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b877e9b-9c85-4a2a-b77b-4a96704f8d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Ensure that Sterilization of medical devices ‚Äî microbiological methods, Part 1\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'What are the standard protocols for sterilizing surgical instruments?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c9b2928a-25f6-4a17-a4c1-7d79bf2ce0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM + VectorDB Response:\n",
      "Hospitals are expected to review their practices and determine what steps are reasonable to safeguard patient information while not impeding the delivery of safe patient care or incurring undue administrative or financial burden as a result of implementing privacy safeguards.\n"
     ]
    }
   ],
   "source": [
    "hospital_ai_fintuned_llm_vectordb_output(user_query = 'How should hospitals handle patient privacy and HIPAA compliance?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d15039-6315-4c84-91f5-485f14a999b1",
   "metadata": {},
   "source": [
    "## ‚úÖ 8. Key Features\n",
    "The fine-tuned agent provides hospital staff, patients, and administrators with accurate guidance on medical norms, safety protocols, and insurance rules.  \n",
    "- **Lightweight** ‚Üí uses `google/flan-t5-small` for fast prototyping and training.  \n",
    "- **Domain-Specific** ‚Üí trained on hospital norms, safety protocols, and insurance regulations.  \n",
    "- **Instruction-Following** ‚Üí learns from JSONL data structured as *instruction ‚Üí input ‚Üí output*.  \n",
    "- **LoRA Fine-Tuning** ‚Üí efficient parameter-efficient training without needing massive compute.  \n",
    "- **Extensible** ‚Üí can scale to larger models (Vicuna, LLaMA, Falcon) with QLoRA for broader coverage.  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2c6c5-5a09-4b14-89c3-1012b73950e0",
   "metadata": {},
   "source": [
    "## ‚úÖ 9. üèÅ Conclusion\n",
    "\n",
    "This project successfully demonstrates the implementation of a domain-specific **AI assistant for hospital regulations, safety protocols, and health insurance norms**. By fine-tuning a lightweight **LLM (`google/flan-t5-small`)** with **LoRA adapters**, we created a system that can interpret and simplify complex medical and insurance guidelines for nurses, doctors, and patients.  \n",
    "\n",
    "The key achievements of this project include:\n",
    "\n",
    "* **Accessible Knowledge**: Converts hospital safety procedures, insurance rules, and compliance standards into easy-to-understand answers.  \n",
    "* **Efficiency with Lightweight Models**: Achieves good performance on specialized tasks without requiring large, resource-intensive LLMs.  \n",
    "* **End-to-End Workflow**: Provides a complete pipeline from **data preparation ‚Üí fine-tuning ‚Üí inference**.  \n",
    "* **Practical Application**: Bridges the gap between **hospital staff, patients, and insurance systems**, ensuring clarity and compliance.  \n",
    "\n",
    "### üöÄ Future Work\n",
    "\n",
    "* **Dataset Expansion**: Include more hospital policies, international healthcare standards, and insurance guidelines.  \n",
    "* **Multi-Language Support**: Extend to non-English hospital environments for wider adoption.  \n",
    "* **Evaluation Benchmarks**: Add systematic evaluation to measure accuracy, coverage, and compliance robustness.  \n",
    "* **Deployment**: Package the model into an API or chatbot for real-time hospital and insurance assistance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9921fb-2f25-49df-a067-9c85290390fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
